# SPARC
This component evaluates tool calls before execution, identifying potential issues and suggesting corrections or transformations across multiple validation layers.


## Overview
The Semantic Pre-execution Analysis for Reliable Calls (SPARC) component provides multi-layered validation for tool calls in agentic systems. It combines syntactic validation, semantic analysis, and intelligent parameter transformations to ensure tool calls are correct, appropriate, and properly formatted before execution.

This component is designed to be used by any tool-calling agent right before tool execution, allowing you to configure metrics and checks based on your specific use case requirements.

### Key Components

1. **Syntactic Validation**: Python-based static analysis of tool call structure (fast and 100% accurate)
2. **Semantic Analysis**: LLM-as-a-judge evaluation of intent alignment and appropriateness
3. **Parameter Transformation**: Code generation for complex value transformations (units, formats, etc.)
4. **Flexible Configuration**: Multiple pre-configured validation profiles for different use cases


## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                SPARC Reflection Middleware                  │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────┐  ┌──────────────┐  ┌─────────────────────┐ │
│  │   Static    │  │   Semantic   │  │   Transformation    │ │
│  │ Validation  │  │  Analysis    │  │    Validation       │ │
│  │             │  │              │  │                     │ │
│  │ (Python)    │  │ (LLM based)  │  │    (LLM based)      │ │
│  └─────────────┘  └──────────────┘  └─────────────────────┘ │
├─────────────────────────────────────────────────────────────┤
│                    LLMEvalKit Integration                   │
│ ┌─────────────────────────────────────────────────────────┐ │
│ │                   Reflection Pipeline                   │ │
│ │  • Metrics Engine    • LLM Provider    • Result Proc    │ │
│ └─────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

### Input Format

SPARC expects three main inputs in OpenAI-compatible formats:
- List of messages representing the conversation context
- Array of tool specifications following OpenAI function calling format
- The tool call generated by your agent that needs validation

## Results

### Tau-bench Airline Domain Evaluation

We evaluated the SPARC Middleware on the Tau-bench airline domain to demonstrate its effectiveness in improving agent performance. In this experiment, we tested the reflection component by adding a reflection step before each tool call. If the tool call was incorrect, the reflection explanation and correction suggestions were returned to the agent to revise the tool call. Otherwise, the tool call was executed as usual.

The results below compare the regular agent (without reflection) and the enhanced version with reflection. The reflection includes both syntactic validation and fast track semantic validation (2 general metrics: hallucination detection and agentic constraint satisfaction).

The experiment was conducted across multiple agent models - GPT-4o, GPT-4o Mini, and Mistral Large - with various reflection models, including a case where the agent is GPT-4o Mini and the reflector is a stronger model like GPT-4o.

### Metrics Definition
- **Average Reward (Pass^1)**: The mean score across all tasks, reflecting overall agent performance
- **Pass^k**: K successful task attempts out of all possible attempts (≥k)

### Results Table

| Model | Metric | Without Reflection | With Reflection | Improvement (%) |
|-------|--------|-------------------|-----------------|----------------|
| GPT-4o | Average Reward | 0.47 | 0.485 | +3.19% |
| GPT-4o | Pass^1 | 0.47 | 0.485 | +3.19% |
| GPT-4o | Pass^2 | 0.3567 | 0.38 | +6.54% |
| GPT-4o | Pass^3 | 0.3 | 0.335 | +11.67% |
| GPT-4o | Pass^4 | 0.26 | 0.3 | +15.38% |
| GPT-4o Mini (reflection by GPT-4o) | Average Reward | 0.175 | 0.185 | +5.71% |
| GPT-4o Mini (reflection by GPT-4o) | Pass^1 | 0.175 | 0.185 | +5.71% |
| GPT-4o Mini (reflection by GPT-4o) | Pass^2 | 0.0833 | 0.1067 | +28.05% |
| GPT-4o Mini (reflection by GPT-4o) | Pass^3 | 0.04 | 0.085 | +112.50% |
| GPT-4o Mini (reflection by GPT-4o) | Pass^4 | 0.02 | 0.08 | +300.00% |
| Mistral Large (35 steps) | Average Reward | 0.08 | 0.1 | +25.00% |
| Mistral Large (35 steps) | Pass^1 | 0.08 | 0.1 | +25.00% |
| Mistral Large (35 steps) | Pass^2 | 0.0133 | 0.0333 | +150.38% |
| Mistral Large (35 steps) | Pass^3 | 0 | 0.01 | — |
| Mistral Large (35 steps) | Pass^4 | 0 | 0 | — |
| Mistral Large (60 steps) | Average Reward | 0.12 | 0.13 | +8.33% |
| Mistral Large (60 steps) | Pass^1 | 0.12 | 0.13 | +8.33% |
| Mistral Large (60 steps) | Pass^2 | 0.0367 | 0.0667 | +81.84% |
| Mistral Large (60 steps) | Pass^3 | 0.01 | 0.045 | +350.00% |
| Mistral Large (60 steps) | Pass^4 | 0 | 0.04 | — |

### Key Findings

1. **Consistent Improvement in Multi-Attempt Success**: The reflection mechanism shows significant improvements in Pass^2, Pass^3, and Pass^4 metrics across all models, indicating better performance when agents have multiple attempts to solve tasks.

2. **Model-Dependent Benefits**: While GPT-4o shows consistent improvements across all metrics, smaller models like GPT-4o Mini benefit more when reflection is performed by a stronger model (GPT-4o) rather than self-reflection.

3. **Substantial Gains for Weaker Models**: Mistral Large shows dramatic improvements, particularly in higher-order Pass metrics, demonstrating that reflection is especially valuable for models that initially struggle with tool calling accuracy.

### RewardBench Evaluation

We also evaluated the middleware on [RewardBench](https://huggingface.co/datasets/ibm-research/fc-reward-bench), a benchmark designed to evaluate reward model performance in function-calling tasks. The benchmark features 1,500 unique user inputs derived from the single-turn splits of the BFCL-v3 dataset, with each input paired with both correct and incorrect function calls.

**Configuration**: llama-4-maverick model using fast track validation (2 general metrics: hallucination detection and agentic constraint satisfaction)

| Model | TPR | TNR | Accuracy | Time/Sample |
|-------|-----|-----|----------|-------------|
| SPARC Reflector | 84.33% | 91.87% | 88.10% | 2.5 seconds |
| Granite-Guardian-3.2-5B | 37.33% | 20.60% | 54.27% | 50 milliseconds |
| FORM-1.5B | 75.40% | 71.07% | 73.23% | 12 milliseconds |
| FORM-3B | 64.27% | 86.60% | 75.43% | 25 milliseconds |

**Important Note**: The SPARC Reflector is fundamentally different from traditional reward models. While reward models (Granite-Guardian, FORM-1.5B, FORM-3B) produce continuous scores between 0 and 1, the reflector focuses on:

- **Error Localization**: Identifying specific issues in tool calls
- **Evidence Provision**: Explaining why a tool call is problematic
- **Correction Suggestions**: Providing actionable recommendations for fixes
- **Configurable Output**: Users can control output verbosity (explanation-only, evidence, corrections) to balance detail vs. processing time

This comprehensive analysis requires generating more tokens than simple reward scoring, which explains the longer processing time. However, the reflector's detailed feedback enables agents to actually correct their tool calls rather than just knowing they made an error.

### BFCL-v3 Detection Performance

We evaluated the reflector's ability to distinguish between correct and incorrect tool calls on the BFCL-v3 benchmark. The slow-track reflector (which includes multiple metrics and unit transformation checks) was tested on tool calls generated using Granite Function Calling 20B and compared against ground truth labels.

#### Single-Turn Performance

| Metric | Score |
|--------|-------|
| **Generator Model** | Granite Function Calling 20b |
| **Reflector Model** | Llama-3-3-70b |
| **Accuracy** | 0.872 |
| **Precision** | 0.823 |
| **Recall** | 0.765 |
| **F1 Score** | 0.793 |

#### Multi-Turn Performance

| Metric | Score |
|--------|-------|
| **Generator Model** | Granite Function Calling 20b |
| **Reflector Model** | Phi-4 |
| **Accuracy** | 0.866 |
| **Precision** | 0.944 |
| **Recall** | 0.905 |
| **F1 Score** | 0.924 |

These results demonstrate the reflector's strong ability to accurately identify problematic tool calls across both single-turn and multi-turn scenarios. The multi-turn configuration shows particularly high precision (94.4%) and recall (90.5%), indicating excellent performance in complex conversational contexts where tool calls may depend on previous interactions.


## Getting Started
Refer to this [README](https://github.com/AgentToolkit/agent-lifecycle-toolkit/blob/main/altk/pre_tool_reflection_toolkit/sparc/README.md) for instructions on how to get started with the code. See an example in action [here]( https://github.com/AgentToolkit/agent-lifecycle-toolkit/blob/main/examples/langgraph_agent_sparc_example.py)
